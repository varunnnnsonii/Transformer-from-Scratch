


# Transformerâ€‘fromâ€‘Scratch

A **fromâ€‘scratch implementation of an autoregressive Transformer language model** in PyTorch â€” no reliance on highâ€‘level libraries like Hugging Face Transformers.  
Designed as a **learning project** to understand how a Transformer language model (similar to GPT) works internally, including **multiâ€‘head selfâ€‘attention, positional encodings, training loop, and text generation**.

---

## ğŸš€ Project Overview

This repository contains a complete implementation of a Transformerâ€‘style **autoregressive language model** built manually in PyTorch, inspired by foundational concepts from the original Transformer architecture (â€œAttention Is All You Needâ€) and simplified implementations (e.g., nanoGPT examples). :contentReference[oaicite:0]{index=0}

The model is trained on a plainâ€‘text dataset (e.g., a sales textbook) and learns to predict the next token given a context window (`context_length`). Once trained, it can generate coherent text continuations autoregressively.

---

## ğŸ“¦ Repository Structure

```

Transformerâ€‘fromâ€‘Scratch/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales_textbook.txt    # Training data
â”œâ”€â”€ transformer.py            # Core model definitions
â”œâ”€â”€ train.py                  # Training & evaluation loop
â”œâ”€â”€ generate.py               # Text generation script
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ README.md                 # This document
â””â”€â”€ .gitignore

````

> *Note:* Some scripts may be combined in your current setup (e.g., training and model classes in a single file). You can split them into separate modules for clarity.

---

## ğŸ§  Key Features

âœ”ï¸ **Fromâ€‘scratch implementation** of core Transformer components  
âœ”ï¸ **Multiâ€‘head selfâ€‘attention** with causal masking  
âœ”ï¸ **Feedâ€‘forward neural networks** with residual connections  
âœ”ï¸ **Sinusoidal positional encoding**  
âœ”ï¸ **Autoregressive text generation**  
âœ”ï¸ Training and validation on custom datasets  

---

## ğŸ› ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/varunnnnsonii/Transformerâ€‘fromâ€‘Scratch.git
cd Transformerâ€‘fromâ€‘Scratch
````

Install dependencies:

```bash
pip install -r requirements.txt
```

Typical `requirements.txt`:

```
torch
tiktoken
requests
```

---

## ğŸ“˜ Usage

### ğŸ“¥ Prepare Dataset

The project expects a text corpus in:

```
data/sales_textbook.txt
```

If not present, the training script will automatically download it from HuggingFace.

---

### ğŸ“Š Train Model

To train the model, run:

```bash
python train.py
```

This script will:

âœ” Load and tokenize the text
âœ” Split into training/validation sets
âœ” Train the Transformer model with AdamW
âœ” Print train/validation loss periodically
âœ” Save model checkpoint (`modelâ€‘ckpt.pt`)

---

### ğŸŒ€ Generate Text

After training, you can generate text:

```bash
python generate.py --start "The salesperson"
```

This generates continuation tokens from a given prompt using the trained model.

---

## ğŸ§© How It Works â€” Concept Breakdown

### ğŸ”¹ 1. Tokenization

Uses the TikToken tokenizer (`cl100k_base`) to convert raw text into token IDs.
These tokens are then converted to PyTorch tensors for batching and context windows.

---

### ğŸ”¹ 2. Positional Encoding

Implements **sinusoidal positional encodings** to provide the model with token order information â€” a requirement since Transformers do not inherently model order. ([Wikipedia][1])

---

### ğŸ”¹ 3. Attention & Multiâ€‘Head Attention

Attention computes similarity between queries & keys, then weights values accordingly.
Multiâ€‘head attention runs several attention â€œheadsâ€ in parallel to capture diverse patterns.

---

### ğŸ”¹ 4. Transformer Blocks

Each block has:

* LayerNorm
* Multiâ€‘head selfâ€‘attention with causal masking
* Feedâ€‘forward network
* Skip residual connections

This structure enables efficient learning and stable gradients.

---

### ğŸ”¹ 5. Autoregressive Generation

During inference:

1. Take prompt tokens
2. Crop to modelâ€™s context length
3. Compute logits
4. Sample next token from softmax distribution
5. Append and repeat

---

## ğŸ“ˆ Training Details

| Hyperparameter   | Value |
| ---------------- | ----- |
| Batch Size       | 4     |
| Context Length   | 16    |
| Model Dimension  | 64    |
| Number of Blocks | 8     |
| Attention Heads  | 4     |
| Dropout          | 0.1   |
| Learning Rate    | 1eâ€‘3  |
| Max Iterations   | 5000  |
| Eval Interval    | 50    |

---

## ğŸ“Œ Tips & Best Practices

âœ… **Save checkpoints frequently** to avoid losing training progress
âœ… **Use GPU (CUDA)** if available for faster training
âœ… Batch size and context length can be increased for richer context learning

---

## ğŸ§ª Results & Examples

After training, an example generation might look like:

```
The salesperson to identify the other cost savings interaction â€¦
```

(Your output will vary with model performance and training length.)

---

## ğŸ“œ License

Licensed under MIT License.
Feel free to use and improve this implementation.

---

## ğŸ™Œ Acknowledgements

Inspired by many educational transformerâ€‘fromâ€‘scratch projects and tutorials that aim to demystify transformer internals. 

---

## ğŸ¤ Contributing

Contributions are welcome!
Feel free to:

* Add CLI flags for hyperparameters
* Split scripts into modules
* Add logging & visualization of training
* Introduce more advanced sampling (topâ€‘k / topâ€‘p)

---
